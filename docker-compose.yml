---

services:
  verba:
    build:
      context: ./
      dockerfile: Dockerfile
    ports:
      - 8000:8000
    environment:
      - WEAVIATE_URL_VERBA=http://weaviate:8080
      - OPENAI_API_KEY=$OPENAI_API_KEY
      - COHERE_API_KEY=$COHERE_API_KEY
      - OLLAMA_URL=http://host.docker.internal:11434
      - OLLAMA_MODEL=$OLLAMA_MODEL
      - OLLAMA_EMBED_MODEL=$OLLAMA_EMBED_MODEL
      - UNSTRUCTURED_API_KEY=$UNSTRUCTURED_API_KEY
      - UNSTRUCTURED_API_URL=$UNSTRUCTURED_API_URL
      - GITHUB_TOKEN=$GITHUB_TOKEN

    volumes:
      - ./data:/data/
    depends_on:
      weaviate:
        condition: service_healthy
    healthcheck:
      test: wget --no-verbose --tries=3 --spider http://localhost:8000 || exit 1
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 10s

  weaviate:
    command:
      - --host
      - 0.0.0.0
      - --port
      - '8080'
      - --scheme
      - http
    image: cr.weaviate.io/semitechnologies/weaviate:1.26.1  # Fixed version for stability
    ports:
      - 8080:8080
      - 50051:50051  # gRPC port for faster communication
    volumes:
      - weaviate_data:/var/lib/weaviate
    restart: on-failure:0
    healthcheck:
      test: wget --no-verbose --tries=3 --spider http://localhost:8080/v1/.well-known/ready || exit 1
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 15s
    environment:
      # Core Configuration
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      QUERY_DEFAULTS_LIMIT: 25
      DEFAULT_VECTORIZER_MODULE: 'none'  # We handle vectorization in Python
      
      # Module Configuration
      ENABLE_MODULES: 'text2vec-openai,text2vec-cohere,text2vec-huggingface,generative-openai,generative-cohere,generative-anthropic'
      
      # OpenAI Module Configuration (for vectorization if needed)
      OPENAI_APIKEY: ${OPENAI_API_KEY:-}
      
      # Anthropic Module Configuration
      ANTHROPIC_APIKEY: ${ANTHROPIC_API_KEY:-}
      
      # Cohere Module Configuration
      COHERE_APIKEY: ${COHERE_API_KEY:-}
      
      # Google Module Configuration (for Gemini)
      GOOGLE_APIKEY: ${GOOGLE_API_KEY:-}
      
      # Performance Configuration
      GOMEMLIMIT: '6GiB'
      GOMAXPROCS: '4'
      LIMIT_RESOURCES: 'false'
      
      # Cluster Configuration
      CLUSTER_HOSTNAME: 'node1'
      AUTOSCHEMA_ENABLED: 'false'  # We manage schema explicitly

volumes:
  weaviate_data: {}
...